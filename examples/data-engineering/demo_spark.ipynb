{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7c1fdbf-38b9-4fc4-8329-6e3cf7c4cd30",
   "metadata": {},
   "source": [
    "## Problem:  Given a dataset of bike trips containing the location with geo-spatial coordinates, compute the total distance commuted by the users collectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39042d46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T09:01:38.177062Z",
     "start_time": "2024-01-30T09:01:37.960412Z"
    }
   },
   "source": [
    "#### The dataset is taken from https://github.com/danielbeach/data-engineering-practice/tree/main/Exercises/Exercise-6/data\n",
    "\n",
    "#### It is possible to compute the distance between two geo-spatial coordinates (lat-long pair). \n",
    "Refer https://www.movable-type.co.uk/scripts/latlong.html for the formula."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df95551c",
   "metadata": {},
   "source": [
    "### Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be409e6a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T10:00:01.042352Z",
     "start_time": "2024-01-30T10:00:00.437890Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd9d47e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T10:01:11.191039Z",
     "start_time": "2024-01-30T10:01:11.185321Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add progress bar to pandas apply() functions\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21b66204",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T11:24:55.733910Z",
     "start_time": "2024-01-30T11:24:55.724910Z"
    }
   },
   "outputs": [],
   "source": [
    "# compute the 'haversine' distance in meters between two geo positions\n",
    "# Refer https://www.movable-type.co.uk/scripts/latlong.html for the formula.\n",
    "def distance(lat1, lon1, lat2, lon2):\n",
    "    R = 6371e3; # radius of Earth in metres\n",
    "    φ1 = lat1 * math.pi/180; # φ, λ in radians\n",
    "    φ2 = lat2 * math.pi/180;\n",
    "    Δφ = (lat2-lat1) * math.pi/180;\n",
    "    Δλ = (lon2-lon1) * math.pi/180;\n",
    "\n",
    "    a = math.sin(Δφ/2) * math.sin(Δφ/2) + math.cos(φ1) * math.cos(φ2) * math.sin(Δλ/2) * math.sin(Δλ/2);\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a));\n",
    "\n",
    "    d = R * c; # distance in metres\n",
    "    return math.nan if math.isnan(d) else int(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0f45d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T09:58:56.476334Z",
     "start_time": "2024-01-30T09:58:56.467968Z"
    }
   },
   "source": [
    "#### load the data file into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5027b52d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T10:25:59.729198Z",
     "start_time": "2024-01-30T10:25:59.723236Z"
    }
   },
   "outputs": [],
   "source": [
    "DATAFILE = 'Divvy_Trips_2020_Q1.xlsx'\n",
    "DATAFILE_PQ = 'Divvy_Trips_2020_Q1.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac3aa80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyarrow\n",
      "  Downloading pyarrow-19.0.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Downloading pyarrow-19.0.1-cp312-cp312-manylinux_2_28_x86_64.whl (42.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyarrow\n",
      "Successfully installed pyarrow-19.0.1\n"
     ]
    }
   ],
   "source": [
    "# !pip install pyarrow\n",
    "# !pip install pyspark\n",
    "# !pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e84e990",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T13:06:49.048231Z",
     "start_time": "2024-01-30T13:06:46.895722Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ride_id</th>\n",
       "      <th>rideable_type</th>\n",
       "      <th>started_at</th>\n",
       "      <th>ended_at</th>\n",
       "      <th>start_station_name</th>\n",
       "      <th>start_station_id</th>\n",
       "      <th>end_station_name</th>\n",
       "      <th>end_station_id</th>\n",
       "      <th>start_lat</th>\n",
       "      <th>start_lng</th>\n",
       "      <th>end_lat</th>\n",
       "      <th>end_lng</th>\n",
       "      <th>member_casual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EACB19130B0CDA4A</td>\n",
       "      <td>docked_bike</td>\n",
       "      <td>2020-01-21 20:06:59</td>\n",
       "      <td>2020-01-21 20:14:30</td>\n",
       "      <td>Western Ave &amp; Leland Ave</td>\n",
       "      <td>239</td>\n",
       "      <td>Clark St &amp; Leland Ave</td>\n",
       "      <td>326.0</td>\n",
       "      <td>41.9665</td>\n",
       "      <td>-87.6884</td>\n",
       "      <td>41.9671</td>\n",
       "      <td>-87.6674</td>\n",
       "      <td>member</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8FED874C809DC021</td>\n",
       "      <td>docked_bike</td>\n",
       "      <td>2020-01-30 14:22:39</td>\n",
       "      <td>2020-01-30 14:26:22</td>\n",
       "      <td>Clark St &amp; Montrose Ave</td>\n",
       "      <td>234</td>\n",
       "      <td>Southport Ave &amp; Irving Park Rd</td>\n",
       "      <td>318.0</td>\n",
       "      <td>41.9616</td>\n",
       "      <td>-87.666</td>\n",
       "      <td>41.9542</td>\n",
       "      <td>-87.6644</td>\n",
       "      <td>member</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>789F3C21E472CA96</td>\n",
       "      <td>docked_bike</td>\n",
       "      <td>2020-01-09 19:29:26</td>\n",
       "      <td>2020-01-09 19:32:17</td>\n",
       "      <td>Broadway &amp; Belmont Ave</td>\n",
       "      <td>296</td>\n",
       "      <td>Wilton Ave &amp; Belmont Ave</td>\n",
       "      <td>117.0</td>\n",
       "      <td>41.9401</td>\n",
       "      <td>-87.6455</td>\n",
       "      <td>41.9402</td>\n",
       "      <td>-87.653</td>\n",
       "      <td>member</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C9A388DAC6ABF313</td>\n",
       "      <td>docked_bike</td>\n",
       "      <td>2020-01-06 16:17:07</td>\n",
       "      <td>2020-01-06 16:25:56</td>\n",
       "      <td>Clark St &amp; Randolph St</td>\n",
       "      <td>51</td>\n",
       "      <td>Fairbanks Ct &amp; Grand Ave</td>\n",
       "      <td>24.0</td>\n",
       "      <td>41.8846</td>\n",
       "      <td>-87.6319</td>\n",
       "      <td>41.8918</td>\n",
       "      <td>-87.6206</td>\n",
       "      <td>member</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>943BC3CBECCFD662</td>\n",
       "      <td>docked_bike</td>\n",
       "      <td>2020-01-30 08:37:16</td>\n",
       "      <td>2020-01-30 08:42:48</td>\n",
       "      <td>Clinton St &amp; Lake St</td>\n",
       "      <td>66</td>\n",
       "      <td>Wells St &amp; Hubbard St</td>\n",
       "      <td>212.0</td>\n",
       "      <td>41.8856</td>\n",
       "      <td>-87.6418</td>\n",
       "      <td>41.8899</td>\n",
       "      <td>-87.6343</td>\n",
       "      <td>member</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ride_id rideable_type           started_at             ended_at  \\\n",
       "0  EACB19130B0CDA4A   docked_bike  2020-01-21 20:06:59  2020-01-21 20:14:30   \n",
       "1  8FED874C809DC021   docked_bike  2020-01-30 14:22:39  2020-01-30 14:26:22   \n",
       "2  789F3C21E472CA96   docked_bike  2020-01-09 19:29:26  2020-01-09 19:32:17   \n",
       "3  C9A388DAC6ABF313   docked_bike  2020-01-06 16:17:07  2020-01-06 16:25:56   \n",
       "4  943BC3CBECCFD662   docked_bike  2020-01-30 08:37:16  2020-01-30 08:42:48   \n",
       "\n",
       "         start_station_name start_station_id                end_station_name  \\\n",
       "0  Western Ave & Leland Ave              239           Clark St & Leland Ave   \n",
       "1   Clark St & Montrose Ave              234  Southport Ave & Irving Park Rd   \n",
       "2    Broadway & Belmont Ave              296        Wilton Ave & Belmont Ave   \n",
       "3    Clark St & Randolph St               51        Fairbanks Ct & Grand Ave   \n",
       "4      Clinton St & Lake St               66           Wells St & Hubbard St   \n",
       "\n",
       "  end_station_id start_lat start_lng  end_lat   end_lng member_casual  \n",
       "0          326.0   41.9665  -87.6884  41.9671  -87.6674        member  \n",
       "1          318.0   41.9616   -87.666  41.9542  -87.6644        member  \n",
       "2          117.0   41.9401  -87.6455  41.9402   -87.653        member  \n",
       "3           24.0   41.8846  -87.6319  41.8918  -87.6206        member  \n",
       "4          212.0   41.8856  -87.6418  41.8899  -87.6343        member  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df = pd.read_excel(DATAFILE)\n",
    "#df = df.dropna()\n",
    "#df = df.astype(str)\n",
    "#df.to_parquet(DATAFILE_PQ)\n",
    "\n",
    "# load the data file into a pandas dataframe\n",
    "df = pd.read_parquet(DATAFILE_PQ)\n",
    "# get rid of the empty rows.\n",
    "df = df.dropna()\n",
    "# view the top 5 records.\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea42148",
   "metadata": {},
   "source": [
    "### collect the geo location pairs per record and call the distance function on each record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97643fbd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T13:07:17.043143Z",
     "start_time": "2024-01-30T13:07:07.018813Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/426886 [00:00<?, ?it/s]/tmp/ipykernel_5723/2939739788.py:2: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  lambda x: distance(float(x[0]), float(x[1]), float(x[2]), float(x[3])), axis=1)\n",
      "100%|██████████| 426886/426886 [00:29<00:00, 14337.22it/s]\n"
     ]
    }
   ],
   "source": [
    "df['distance'] = df[['start_lat', 'start_lng', 'end_lat', 'end_lng']].progress_apply(\n",
    "    lambda x: distance(float(x[0]), float(x[1]), float(x[2]), float(x[3])), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67dcb768",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T10:43:53.843685Z",
     "start_time": "2024-01-30T10:43:52.932538Z"
    }
   },
   "source": [
    "### now compute the total by invoking the sum method of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdd53c13",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T13:07:19.925293Z",
     "start_time": "2024-01-30T13:07:19.917966Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trip distance is 789626 kilometers over 426886 trips\n"
     ]
    }
   ],
   "source": [
    "total = df.distance.sum()\n",
    "print('Total trip distance is', int(total/1000), 'kilometers over', df.shape[0], 'trips')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c68c98",
   "metadata": {},
   "source": [
    "### let's look at some of the long distance trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d631e746",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T13:07:24.416526Z",
     "start_time": "2024-01-30T13:07:24.390105Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ride_id</th>\n",
       "      <th>rideable_type</th>\n",
       "      <th>started_at</th>\n",
       "      <th>ended_at</th>\n",
       "      <th>start_station_name</th>\n",
       "      <th>start_station_id</th>\n",
       "      <th>end_station_name</th>\n",
       "      <th>end_station_id</th>\n",
       "      <th>start_lat</th>\n",
       "      <th>start_lng</th>\n",
       "      <th>end_lat</th>\n",
       "      <th>end_lng</th>\n",
       "      <th>member_casual</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>322208</th>\n",
       "      <td>691842BB665D276C</td>\n",
       "      <td>docked_bike</td>\n",
       "      <td>2020-03-10 06:30:00</td>\n",
       "      <td>2020-03-10 17:39:43</td>\n",
       "      <td>Jeffery Blvd &amp; 71st St</td>\n",
       "      <td>11</td>\n",
       "      <td>Broadway &amp; Wilson Ave</td>\n",
       "      <td>293.0</td>\n",
       "      <td>41.7666</td>\n",
       "      <td>-87.5764</td>\n",
       "      <td>41.9652</td>\n",
       "      <td>-87.6581</td>\n",
       "      <td>casual</td>\n",
       "      <td>23096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387373</th>\n",
       "      <td>858A96493DC2021A</td>\n",
       "      <td>docked_bike</td>\n",
       "      <td>2020-03-25 16:40:56</td>\n",
       "      <td>2020-03-25 18:46:45</td>\n",
       "      <td>Sheridan Rd &amp; Montrose Ave</td>\n",
       "      <td>231</td>\n",
       "      <td>South Shore Dr &amp; 67th St</td>\n",
       "      <td>355.0</td>\n",
       "      <td>41.9617</td>\n",
       "      <td>-87.6546</td>\n",
       "      <td>41.7736</td>\n",
       "      <td>-87.5675</td>\n",
       "      <td>casual</td>\n",
       "      <td>22124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403195</th>\n",
       "      <td>6E10F2D3C25CCED5</td>\n",
       "      <td>docked_bike</td>\n",
       "      <td>2020-03-01 11:05:16</td>\n",
       "      <td>2020-03-01 12:51:10</td>\n",
       "      <td>Eberhart Ave &amp; 61st St</td>\n",
       "      <td>431</td>\n",
       "      <td>Kedzie Ave &amp; Leland Ave</td>\n",
       "      <td>476.0</td>\n",
       "      <td>41.7841</td>\n",
       "      <td>-87.6133</td>\n",
       "      <td>41.9667</td>\n",
       "      <td>-87.7081</td>\n",
       "      <td>casual</td>\n",
       "      <td>21768</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ride_id rideable_type           started_at  \\\n",
       "322208  691842BB665D276C   docked_bike  2020-03-10 06:30:00   \n",
       "387373  858A96493DC2021A   docked_bike  2020-03-25 16:40:56   \n",
       "403195  6E10F2D3C25CCED5   docked_bike  2020-03-01 11:05:16   \n",
       "\n",
       "                   ended_at          start_station_name start_station_id  \\\n",
       "322208  2020-03-10 17:39:43      Jeffery Blvd & 71st St               11   \n",
       "387373  2020-03-25 18:46:45  Sheridan Rd & Montrose Ave              231   \n",
       "403195  2020-03-01 12:51:10      Eberhart Ave & 61st St              431   \n",
       "\n",
       "                end_station_name end_station_id start_lat start_lng  end_lat  \\\n",
       "322208     Broadway & Wilson Ave          293.0   41.7666  -87.5764  41.9652   \n",
       "387373  South Shore Dr & 67th St          355.0   41.9617  -87.6546  41.7736   \n",
       "403195   Kedzie Ave & Leland Ave          476.0   41.7841  -87.6133  41.9667   \n",
       "\n",
       "         end_lng member_casual  distance  \n",
       "322208  -87.6581        casual     23096  \n",
       "387373  -87.5675        casual     22124  \n",
       "403195  -87.7081        casual     21768  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the records with more than 20km trips\n",
    "df[df.distance>20000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a737abd4",
   "metadata": {},
   "source": [
    "## Let's solve it using PySpark now, hopefully using parallel processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbd1e53",
   "metadata": {},
   "source": [
    "### import the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "821e7787",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T10:51:42.869673Z",
     "start_time": "2024-01-30T10:51:42.863361Z"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255f3d21",
   "metadata": {},
   "source": [
    "### create the spark context, which will create the spark backbone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69875740",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T10:54:53.368209Z",
     "start_time": "2024-01-30T10:54:50.012320Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/09 09:35:44 WARN Utils: Your hostname, codespaces-6bfd1c resolves to a loopback address: 127.0.0.1; using 10.0.4.178 instead (on interface eth0)\n",
      "25/04/09 09:35:44 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/09 09:35:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/09 09:35:58 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "sc = pyspark.SparkContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d26b173",
   "metadata": {},
   "source": [
    "#### We can monitor the operation via http://localhost:4040\n",
    "### let's create Spark Dataframe from the pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9358a108",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T13:08:24.747207Z",
     "start_time": "2024-01-30T13:07:29.184479Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/09 09:37:07 WARN TaskSetManager: Stage 0 contains a task of very large size (26361 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/04/09 09:37:12 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 0 (TID 0): Attempting to kill Python Worker\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------+-------------------+-------------------+--------------------+----------------+--------------------+--------------+---------+---------+-------+--------+-------------+--------+\n",
      "|         ride_id|rideable_type|         started_at|           ended_at|  start_station_name|start_station_id|    end_station_name|end_station_id|start_lat|start_lng|end_lat| end_lng|member_casual|distance|\n",
      "+----------------+-------------+-------------------+-------------------+--------------------+----------------+--------------------+--------------+---------+---------+-------+--------+-------------+--------+\n",
      "|EACB19130B0CDA4A|  docked_bike|2020-01-21 20:06:59|2020-01-21 20:14:30|Western Ave & Lel...|             239|Clark St & Leland...|         326.0|  41.9665| -87.6884|41.9671|-87.6674|       member|    1737|\n",
      "|8FED874C809DC021|  docked_bike|2020-01-30 14:22:39|2020-01-30 14:26:22|Clark St & Montro...|             234|Southport Ave & I...|         318.0|  41.9616|  -87.666|41.9542|-87.6644|       member|     833|\n",
      "|789F3C21E472CA96|  docked_bike|2020-01-09 19:29:26|2020-01-09 19:32:17|Broadway & Belmon...|             296|Wilton Ave & Belm...|         117.0|  41.9401| -87.6455|41.9402| -87.653|       member|     620|\n",
      "|C9A388DAC6ABF313|  docked_bike|2020-01-06 16:17:07|2020-01-06 16:25:56|Clark St & Randol...|              51|Fairbanks Ct & Gr...|          24.0|  41.8846| -87.6319|41.8918|-87.6206|       member|    1231|\n",
      "|943BC3CBECCFD662|  docked_bike|2020-01-30 08:37:16|2020-01-30 08:42:48|Clinton St & Lake St|              66|Wells St & Hubbar...|         212.0|  41.8856| -87.6418|41.8899|-87.6343|       member|     783|\n",
      "|6D9C8A6938165C11|  docked_bike|2020-01-10 12:33:05|2020-01-10 12:37:54|Wells St & Hubbar...|             212|Desplaines St & R...|          96.0|  41.8899| -87.6343|41.8846|-87.6446|       member|    1036|\n",
      "|31EB9B8F406D4C82|  docked_bike|2020-01-10 13:07:35|2020-01-10 13:12:24|Desplaines St & R...|              96|Wells St & Hubbar...|         212.0|  41.8846| -87.6446|41.8899|-87.6343|       member|    1036|\n",
      "|A2B24E3F9C9720E3|  docked_bike|2020-01-10 07:24:53|2020-01-10 07:29:50|Desplaines St & R...|              96|Wells St & Hubbar...|         212.0|  41.8846| -87.6446|41.8899|-87.6343|       member|    1036|\n",
      "|5E3F01E1441730B7|  docked_bike|2020-01-31 16:37:16|2020-01-31 16:42:11|Wells St & Hubbar...|             212|Desplaines St & R...|          96.0|  41.8899| -87.6343|41.8846|-87.6446|       member|    1036|\n",
      "|19DC57F7E3140131|  docked_bike|2020-01-31 09:39:17|2020-01-31 09:42:40|  Clark St & Lake St|              38|Orleans St & Merc...|         100.0|   41.886| -87.6309|41.8882|-87.6364|       member|     516|\n",
      "|8639202DD9FD9A41|  docked_bike|2020-01-07 22:46:52|2020-01-07 22:50:08|Wilton Ave & Belm...|             117|Clark St & Newpor...|         632.0|  41.9402|  -87.653|41.9445|-87.6547|       member|     498|\n",
      "|9E74E3BB4FFAB93A|  docked_bike|2020-01-08 16:02:44|2020-01-08 16:09:04|LaSalle St & Illi...|             181|Clinton St & Wash...|          91.0|  41.8908| -87.6317|41.8834|-87.6412|       member|    1138|\n",
      "|3B8B2E2F29B63597|  docked_bike|2020-01-08 09:33:13|2020-01-08 09:42:02|Clinton St & Wash...|              91|LaSalle St & Illi...|         181.0|  41.8834| -87.6412|41.8908|-87.6317|       member|    1138|\n",
      "|0F8517F8D21287D2|  docked_bike|2020-01-23 09:38:11|2020-01-23 09:45:56|Clinton St & Wash...|              91|LaSalle St & Illi...|         181.0|  41.8834| -87.6412|41.8908|-87.6317|       member|    1138|\n",
      "|15A91638FAEC2641|  docked_bike|2020-01-28 20:52:50|2020-01-28 21:09:30|California Ave & ...|             123|Marshfield Ave & ...|          58.0|  41.9227| -87.6972| 41.916|-87.6689|       member|    2457|\n",
      "|E45104F1ED756AF7|  docked_bike|2020-01-07 17:06:14|2020-01-07 17:42:01|Franklin St & Jac...|              36|Lincoln Ave & Div...|         152.0|  41.8777| -87.6353|41.9322|-87.6586|       member|    6359|\n",
      "|219541294624C4B7|  docked_bike|2020-01-07 08:38:55|2020-01-07 09:11:40|Wilton Ave & Dive...|              13|Franklin St & Jac...|          36.0|  41.9324| -87.6527|41.8777|-87.6353|       member|    6250|\n",
      "|013862D47804B9A4|  docked_bike|2020-01-06 17:16:14|2020-01-06 17:48:21|Franklin St & Jac...|              36|Lincoln Ave & Div...|         152.0|  41.8777| -87.6353|41.9322|-87.6586|       member|    6359|\n",
      "|00678BB4A8438651|  docked_bike|2020-01-06 08:44:31|2020-01-06 09:21:28|Lincoln Ave & Div...|             152|Franklin St & Jac...|          36.0|  41.9322| -87.6586|41.8777|-87.6353|       member|    6359|\n",
      "|A854F81611B5A5C0|  docked_bike|2020-01-19 12:04:10|2020-01-19 12:10:18|Kingsbury St & Ki...|             133|Clark St & Chicag...|         337.0|  41.8892| -87.6385|41.8968|-87.6309|       member|    1053|\n",
      "+----------------+-------------+-------------------+-------------------+--------------------+----------------+--------------------+--------------+---------+---------+-------+--------+-------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    " \n",
    "# Building the SparkSession and name\n",
    "# it :'pandas to spark'\n",
    "spark = SparkSession.builder.appName(\"pandas to spark\").getOrCreate()\n",
    " \n",
    "# create DataFrame\n",
    "df_spark = spark.createDataFrame(df)\n",
    " \n",
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a850c71",
   "metadata": {},
   "source": [
    "### let's compute the distance from the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3028e0da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T14:11:23.602809Z",
     "start_time": "2024-01-30T14:11:23.595657Z"
    }
   },
   "outputs": [],
   "source": [
    "dist = df_spark.rdd.map(lambda x: distance(float(x[8]), float(x[9]), float(x[10]), float(x[11])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d66a58",
   "metadata": {},
   "source": [
    "### let's now compute the total distance by reducing the RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87b0e42d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T14:11:30.010358Z",
     "start_time": "2024-01-30T14:11:26.090919Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/09 09:37:19 WARN TaskSetManager: Stage 1 contains a task of very large size (26361 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "total_distance = dist.reduce(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fea0635",
   "metadata": {},
   "source": [
    "### report the findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2e76ae9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T14:11:34.348534Z",
     "start_time": "2024-01-30T14:11:33.076658Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/09 09:37:31 WARN TaskSetManager: Stage 2 contains a task of very large size (26361 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 2:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trip distance is 789626 kilometers over 426886 trips\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "count = df_spark.count()\n",
    "print('Total trip distance is', int(total_distance/1000), 'kilometers over', count, 'trips')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa9357f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T11:26:47.955620Z",
     "start_time": "2024-01-30T11:26:44.605220Z"
    }
   },
   "source": [
    "### Let's try another way of doing this in Spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db900a2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T12:40:46.901223Z",
     "start_time": "2024-01-30T12:40:43.312343Z"
    }
   },
   "source": [
    "#### let's get the lat lon values from the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ab556a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T13:12:50.121397Z",
     "start_time": "2024-01-30T13:12:50.096207Z"
    }
   },
   "outputs": [],
   "source": [
    "latlon_records = df[['start_lat', 'start_lng', 'end_lat', 'end_lng']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02330165",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T13:12:52.048301Z",
     "start_time": "2024-01-30T13:12:52.040725Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['41.9665', '-87.6884', '41.9671', '-87.6674'],\n",
       "       ['41.9616', '-87.666', '41.9542', '-87.6644'],\n",
       "       ['41.9401', '-87.6455', '41.9402', '-87.653'],\n",
       "       ...,\n",
       "       ['41.9157', '-87.6346', '41.9035', '-87.6677'],\n",
       "       ['41.891', '-87.6355', '41.8868', '-87.6223'],\n",
       "       ['41.894', '-87.6293', '41.901', '-87.6238']],\n",
       "      shape=(426886, 4), dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latlon_records"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0c04c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T12:35:56.274660Z",
     "start_time": "2024-01-30T12:35:54.933524Z"
    }
   },
   "source": [
    "### let's convert the data in to a RDD.  Here the number of slices is an important parameter that controls the number of jobs that are runnable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf4b38de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T14:07:16.598659Z",
     "start_time": "2024-01-30T14:07:12.028631Z"
    }
   },
   "outputs": [],
   "source": [
    "latlon_rdd = sc.parallelize(latlon_records, numSlices=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1895d1bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T12:40:25.164202Z",
     "start_time": "2024-01-30T12:40:25.157170Z"
    }
   },
   "source": [
    "### let's now run the same job of computing the individual distances followed by the total distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7330c085",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T14:09:33.531788Z",
     "start_time": "2024-01-30T14:09:29.716022Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "total_distance = latlon_rdd \\\n",
    ".map(lambda x: distance(float(x[0]), float(x[1]), float(x[2]), float(x[3]))) \\\n",
    ".reduce(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b4b583",
   "metadata": {},
   "source": [
    "### report the findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "43b4843d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T14:10:18.601333Z",
     "start_time": "2024-01-30T14:10:15.430165Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:====================================================>   (93 + 2) / 100]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trip distance is 789626 kilometers over 426886 trips\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "count = latlon_rdd.count()\n",
    "print('Total trip distance is', int(total_distance/1000), 'kilometers over', count, 'trips')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267b052a",
   "metadata": {},
   "source": [
    "### Let's look at another example of parallel processing files using Spark\n",
    "\n",
    "## Problem: Given a folder of images, OCR them and compute the token distribution\n",
    "\n",
    "* Convert the image to text\n",
    "* combine the texts into a large blob\n",
    "* tokenize the text into token seperated by whitespaces\n",
    "* compute the number of unique tokens with their respect counts\n",
    "* save the output in a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "45a86f70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T14:29:54.394168Z",
     "start_time": "2024-01-30T14:29:54.388142Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "FOLDER = 'funsd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d0c4cce3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T14:32:53.819729Z",
     "start_time": "2024-01-30T14:32:53.809414Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['funsd/86263525.png',\n",
       " 'funsd/11875011.png',\n",
       " 'funsd/0012529284.png',\n",
       " 'funsd/93351929_93351931.png',\n",
       " 'funsd/0060068489.png']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_files = list(map(lambda x: FOLDER + '/' + x.name, Path(FOLDER).glob('*.*')))\n",
    "list_of_files[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bcf27d",
   "metadata": {},
   "source": [
    "### create a function to invoke the tesseract command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "af333b00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-31T06:34:35.104413Z",
     "start_time": "2024-01-31T06:34:35.096426Z"
    }
   },
   "outputs": [],
   "source": [
    "import subprocess as sp\n",
    "import os\n",
    "my_env = os.environ.copy()\n",
    "my_env[\"OMP_THREAD_LIMIT\"] = '1'\n",
    "\n",
    "def ocr_task(path):\n",
    "    # invoke the tesseract command to run OCR on the input image\n",
    "    # set the output to go to stdout so that we can collect it in memory.\n",
    "    result = sp.run(['tesseract', path, '-'], \n",
    "                     stdout=sp.PIPE, stderr=sp.PIPE, \n",
    "                     check=True, text=True,\n",
    "                     env=my_env)\n",
    "    # check if the command executed without errors\n",
    "    if result.returncode == 0:\n",
    "        # return the OCR text\n",
    "        return result.stdout\n",
    "    # return blank to filter later.\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61356682",
   "metadata": {},
   "source": [
    "### check if the function is working fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "379d0fa9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-31T06:34:37.274901Z",
     "start_time": "2024-01-31T06:34:36.541667Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\n\\n \\n\\nA.T.Co, Tar & Nicotine Change Fors\\nba 3/24/99\\n. »\\n\\nbrand & Style CARLTON 100\"s FHSP\\n\\n_——_——__Erem____ —_—__e____.\\n\\nTax Micotine_ —tar_ Ricotine—\\n(Mg/cigt) (Mg/cigt) (ig/cigt) (ng/cigt) -\\n\\n3 0.3 2 0.2\\n\\nSignature Cub) Oty\\n\\n \\n\\nNOTE: Use Separate Form For Each Change\\n\\n \\n\\x0c'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ocr_task('funsd/0060308251.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962b1b57",
   "metadata": {},
   "source": [
    "### let's gauge the time taken to run the OCR task in sequential order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f12dfadc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-31T06:35:56.074347Z",
     "start_time": "2024-01-31T06:35:56.069117Z"
    }
   },
   "outputs": [],
   "source": [
    "text_fragments = map(ocr_task, list_of_files[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0bb0af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-31T06:37:53.073829Z",
     "start_time": "2024-01-31T06:35:57.049250Z"
    }
   },
   "outputs": [],
   "source": [
    "all_text = \"\\n\".join(text_fragments)\n",
    "all_text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23d6839",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-31T05:40:05.861884Z",
     "start_time": "2024-01-31T05:40:05.854947Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aaee7747",
   "metadata": {},
   "source": [
    "### let's try to parallelization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4cbb43f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-31T06:35:00.866659Z",
     "start_time": "2024-01-31T06:35:00.855201Z"
    }
   },
   "outputs": [],
   "source": [
    "lof_rdd = sc.parallelize(list_of_files[::-1], numSlices=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd33da8",
   "metadata": {},
   "source": [
    "### we will configure the ocr_task as the mapper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9dfdca9f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-31T06:35:03.052045Z",
     "start_time": "2024-01-31T06:35:03.045816Z"
    }
   },
   "outputs": [],
   "source": [
    "texts = lof_rdd.map(ocr_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e86bda",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-30T15:01:08.840720Z",
     "start_time": "2024-01-30T15:01:08.826622Z"
    }
   },
   "source": [
    "### we will now tokenize each of the texts into an array of tokens\n",
    "#### we use flatMap here which is an equivalent of map() followed by flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "34bf4781",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-31T06:35:04.065726Z",
     "start_time": "2024-01-31T06:35:04.059829Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "# flatmap gets one dimensional array, while map gets an array of array.\n",
    "# as we are interested in counting the unique tokens, we need a flattened array.\n",
    "tokens = texts.flatMap(lambda x: re.findall(r'[A-Za-z\\']+', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b084ab1",
   "metadata": {},
   "source": [
    "### let's convert every token to a tuple (token,1), which we can reduce by key later to get the distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a3ab1f49",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-31T06:35:05.581104Z",
     "start_time": "2024-01-31T06:35:05.574754Z"
    }
   },
   "outputs": [],
   "source": [
    "token_tuples = tokens.map(lambda x: (x,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b5a287",
   "metadata": {},
   "source": [
    "### let's count by key to get the distribution now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee825210",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-31T06:35:44.836550Z",
     "start_time": "2024-01-31T06:35:07.122925Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:============================================>              (6 + 2) / 8]\r"
     ]
    }
   ],
   "source": [
    "token_counts = token_tuples.countByKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed5f09b",
   "metadata": {},
   "source": [
    "### Let's create a dataframe with the estimated token distribution results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "addc9602",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-31T05:47:00.561397Z",
     "start_time": "2024-01-31T05:47:00.550678Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'token_counts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m newdf = pd.DataFrame({\u001b[33m\"\u001b[39m\u001b[33mtokens\u001b[39m\u001b[33m\"\u001b[39m:\u001b[38;5;28mlist\u001b[39m(\u001b[43mtoken_counts\u001b[49m.keys()), \u001b[33m\"\u001b[39m\u001b[33mfreq\u001b[39m\u001b[33m\"\u001b[39m:\u001b[38;5;28mlist\u001b[39m(token_counts.values())})\n",
      "\u001b[31mNameError\u001b[39m: name 'token_counts' is not defined"
     ]
    }
   ],
   "source": [
    "newdf = pd.DataFrame({\"tokens\":list(token_counts.keys()), \"freq\":list(token_counts.values())})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c007f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-31T05:46:58.711031Z",
     "start_time": "2024-01-31T05:46:58.692501Z"
    }
   },
   "outputs": [],
   "source": [
    "newdf.sort_values('freq', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666d759d",
   "metadata": {},
   "source": [
    "### now, save it as a spreadsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481d9bec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-31T05:48:11.356141Z",
     "start_time": "2024-01-31T05:48:09.472933Z"
    }
   },
   "outputs": [],
   "source": [
    "newdf.to_excel('/tmp/output.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e7d86f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fadebe9f",
   "metadata": {},
   "source": [
    "## Let's process Amazon reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d624572",
   "metadata": {},
   "source": [
    "### How to do complex transformation using map functions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa148fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper to read a text file\n",
    "def read_file(path):\n",
    "    with open(path, 'r') as file:\n",
    "        textdata = file.read()\n",
    "        file.close()\n",
    "        return textdata\n",
    "    \n",
    "# create a helper function to view a sample of a text file\n",
    "def view_file(path, length=50, lines=False):\n",
    "    textdata = read_file(path)\n",
    "\n",
    "    # if we need lines, split it and display the required number of lines.\n",
    "    sample = \"\\n\".join(textdata.split(\"\\n\")[:length]) if lines else textdata[:length]\n",
    "        \n",
    "    print(\"TextSize:\", len(textdata), \"\\n\\nSample:\", sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23d0e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAFILE = 'Gourmet_Foods.txt'\n",
    "view_file(DATAFILE, 20, lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274af8e7",
   "metadata": {},
   "source": [
    "### data file is a single archive of reviews.  We need to extract the review/text to construct a dataset for further processing\n",
    "* scan the file for \"review/text:\" pattern and extract the right side of the pattern.\n",
    "* also get the product id, so that we can map the review text to the product id.\n",
    "* let's also pick up the review/score to record the star rating.\n",
    "* now we should have a triplet with (productid, rating, review_text)\n",
    "* if we carefully see, the reviews are seperated by multiple consecutive newlines!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373d6087",
   "metadata": {},
   "source": [
    "### let's read the data and split the data based on consecutive newlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e989f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# read the data file and split by \\n\\n+\n",
    "reviews = re.split(r'\\n\\n+', read_file(DATAFILE))\n",
    "print(\"number of reviews:\", len(reviews))\n",
    "print(reviews[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b885ef9a",
   "metadata": {},
   "source": [
    "### create the parallelizable dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac3ee56",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_rdd = sc.parallelize(reviews, numSlices=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ab67a8",
   "metadata": {},
   "source": [
    "### we can define a function to process each block to extract the triplet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795b65c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(text):\n",
    "    match = re.search('product/productId: (.+)', text)\n",
    "    product_id = match.group(1) if match else \"\"\n",
    "    match = re.search('review/score: (.+)', text)\n",
    "    star_rating = float(match.group(1)) if match else 0.0\n",
    "    match = re.search('review/text: (.+)', text)\n",
    "    review_text = match.group(1) if match else \"\"\n",
    "    return (product_id, star_rating, review_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ade604",
   "metadata": {},
   "source": [
    "### let's extend the beam to include the extraction of triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebae032",
   "metadata": {},
   "outputs": [],
   "source": [
    "triplets = reviews_rdd.map(process).filter(lambda x: x[0] != \"\" and x[1]>0.0 and x[2] != \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ee5d18",
   "metadata": {},
   "source": [
    "### As a task, let's group the data by product id to find the average rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054c4ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's get rid of the texts first and them group the data by key (product id)\n",
    "product_rating = triplets.map(lambda x: (x[0], x[1])).groupByKey().map(lambda p_r: (p_r[0], round(sum(p_r[1])/len(p_r[1]),2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2ae635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the pipeline now.\n",
    "result = product_rating.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54e31e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the result by ratings\n",
    "result_sorted = sorted(result, key=lambda tup: tup[1], reverse=True)\n",
    "result_sorted[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f60e349",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
